---
title: "R05_1_regular_expression"
author: "Jilung Hsieh"
date: "10/16/2019"
output:
  html_document:
    theme: cerulean
    highlight: zenburn
    toc: yes
    toc_float:
      collapsed: no
    df_print: paged
editor_options:
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Loading packages
```{r}
library(tidyverse)
options(stringsAsFactors = F)
options(verbose = T)
options(scipen = 999)
```
# Online tools for detecting regular expression (RE)
- https://regex101.com/
- http://regexr.com/


# RE applications on string operaions
- Replacing specific pattern
    - Replacing all space or new line character to one space or empty string
    - Deleting all number and alphabetic by empty string
    - Deleting all HTML tags
- Detecting if it exists some specific pattern
    - Fitering if one sentence starting with ".
    
- Extracting some words by specific pattern
    - Extracting hashtags in text
    - Extracting timestamp, email, hyperlink, or url in text
    - Extracting number after some specific patterns
- Splitting
    - Splitting string into several parts when detecting some specific pattern successfully.



# Replacing
- `\s` matching any whitespace character including space, tabs, and line breaks
- R uses `\\s`, instead of `\s`.
- `\\d` digit, `\\D` not digit
- `\\s` whitepsace, `\\S` not whitespace
- `\\w` word, `\\W` not word
- `.` matches any
- `[A-Z0-9a-z]` range
- `[ABC]` character set
- `[^ABC]` neglect set (Leadning `^` in `[]` means neglecting)

```{r}
s <- "   123   456   789   "
str_replace_all(s, "\\s", "")
```


# Extracting
- `+` means matching word occurring at least one time, matching longer is better
- `*` means matching word occurring any time, matching longer is better
- `{1,3}` means matching at least one time, but at most 3 times
- `{3}` means matching 3 times exactly.


## by `str_extract()`
- Quantifier
```{r}
pname <- c("pttid1111(kefan)", "pid2(hangfan)")

# extracting at least one words until not word
str_extract(pname, "\\w")
str_extract(pname, "\\w+")
str_extract(pname, "\\w*")
str_extract(pname, "[A-Za-z0-9]+")
str_extract(pname, "[A-Za-z0-9]*")
str_extract(pname, "[A-Za-z0-9]{8}")
str_extract(pname, "[A-Za-z0-9]{1,8}")


# extracting word between ()
# () is controlled vocabulary in regular expression, needing \\( and \\) to specify the character themself
str_extract(pname, "\\(\\w+\\)") %>% str_replace_all("[\\(\\)]", "")
str_extract(pname, "\\(.+\\)") 

```

## by `str_extract` for data frame
- to extract id and nickname
```{r}
data_frame(pname) %>%
    mutate(nickname = str_extract(pname, "\\(\\w+\\)") %>% str_replace_all("[\\(\\)]", "")) %>%
    mutate(id = str_extract(pname, "\\w+")) %>%
    View




```



## by `str_replace()`
- `(\\w+)`的括號代表，我等一下要抽取出這個pattern的字
- `\\(.+\\)`則是把剩下的pattern matching 完
- `\\1`代表我要抽取前面第一組抽取出來的pattern，也就是`(\\w+)`中間的`\\w+`。

```{r}
data_frame(pname) %>%
    mutate(id = str_replace(pname, "(\\w+)\\(.+\\)", "\\1"))
```

## by `tidyr::extract()` for data frame

```{r}
data_frame(pname) %>% 
    tidyr::extract(pname, c("id", "nickname"), "(\\w+)\\((.+)\\)", remove = F)
?tidyr::extract
```

# Detecting with non-greedy = lazy
```{r}
source <- c("<p>Twitter for iphone</p>", "<div><p>Twitter for iphone</p></div>")
data_frame(source) %>% extract(source, "device", "Twitter for (.*)<")
```
```{r}
data_frame(source) %>%
    extract(source, "device", "Twitter for (.*?)<")
```



# Detecting multiple patterns
- https://stackoverflow.com/questions/8020848/how-is-the-and-or-operator-represented-as-in-regular-expressions

```{r}
teststr <- c("B和A是不是男女朋友呢", "C與B是不是在一起呢", "A就是B。")
re1 <- "(.*B.*呢$)"
re2 <- "(.*A.*)"
str_detect(teststr, re1)
str_detect(teststr, re2)
str_detect(teststr, re1) & str_detect(teststr, re2)
str_detect(teststr, re1) | str_detect(teststr, re2)
```

# Extracting nearby words
## Extracting nearby 3 English words
```{r}
string2 <- "..., compelled to defend as never before the hard-charging progressivism and ...."
data_frame(string2) %>%
    mutate(string2 = str_extract(string2, "(\\S+\\s){3}before(\\s\\S+){3}")) %>%
    extract(string2, c("prefix","hit", "tail"), "(.+)(before)(.+)")
```

## Extracting nearby 3 Chinese words
```{r}
string3 <- c("呵呵呵呵呵呵呵我家就住在台灣，那是個美麗的地方",
             "臺灣真是個美麗的地方齁齁齁", 
             "呵呵呵呵呵呵呵我愛台灣臺灣")
str_extract(string3, ".{5}台灣.{5}|.{5}臺灣.{5}")
str_extract(string3, ".{1,5}台灣.{1,5}|.{1,5}臺灣.{1,5}")
```

## Extracting nearby 3 Chinese words
```{r}
df <- data_frame(string3)
df %>% extract(string3, c("pre", "hit", "tail"), "(.{0,5})(台灣|臺灣)(.{0,5})")
```

# Our cases

## Getting the last page of PTT HatePolitics
```{r}
urls <- c("https://www.ptt.cc/bbs/HatePolitics/index4166.html", "https://www.ptt.cc/bbs/HatePolitics/index348.html")

str_extract(urls, "index(\\d+)\\.html")
str_replace(urls, ".*index(\\d+)\\.html", "\\1") %>% as.numeric()
data_frame(urls) %>% extract(urls, "last_page", "index(\\d+)\\.html", remove = F)
```



# Famous cases

## Matching URL
```{r}
pattern <- "^((https?|ftp)://|(www|ftp)\\.)?[a-z0-9-]+(\\.[a-z0-9-]+)+([/?].*)?$"

str_detect("http://www.yahoo.com.tw", pattern, )
str_detect("https://m.facebook.com/story.php?story_fbid=1483357095305703&id=1435979486710131", pattern)
str_detect("https://www.facebook.com/groups/335691256571414/permalink/774316322708903/", pattern)
```

# Practices

## Removing all html tags but keeping  comment content
ANS: `"推 ya870801: 推  218.166.12.10 10/16 15:56"`

```{r}
comment <- '<div class="push"><span class="hl push-tag">推 </span><span class="f3 hl push-userid">ya870801</span><span class="f3 push-content">: 推</span><span class="push-ipdatetime">  218.166.12.10 10/16 15:56</span></div>'

```

## Removing space
```{r}
# Removing space in Chinese sentence
sentence <- c(' 噓 wwHui: 這批安好純   \n  辛苦了  \n 噓 ', '噓 wwHui: 這批安好純   ', '辛苦了  ')

# Removing all space characters

# Removing leading and ending space by str_replace_all()
# You cannot use trimws()

```


