---
title: "R03_5_PTT_scraping_cookie"
author: "Jilung Hsieh"
date: "2019/9/2"
output:
  html_document:
    theme: cerulean
    highlight: zenburn
    toc: yes
    toc_float:
      collapsed: no
    df_print: paged
editor_options:
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# loading packages
```{r}
library(rvest)
library(httr)
library(tidyverse)
options(stringsAsFactors = F)
options(verbose = T)
options(scipen = 999)
```

# GET() html with cookie

## Testing: GET() directly
```{r}
# url
url <- "https://www.ptt.cc/bbs/HatePolitics/index.html"

# Using read_html(), write_html() and browseURL() to examine the link
read_html(url) %>% write_html("test.html")

# Browsing the URL by browseURL()
browseURL("test.html")

```



## Testing: GET() with cookie
```{r}
# GET html with cookie
url <- "https://www.ptt.cc/bbs/HatePolitics/index.html"
response <- GET(url, config = set_cookies("over18" = "1"))

# content() %>% read_html() to an xml_document
response %>%
    content("text") %>%
    read_html() %>%
    write_html("test_with_cookie.html")


# Examining the url
browseURL("test_with_cookie.html")

```

## Code: GET() html with cookie
```{r}
# the url

# GET() with cookie and convert to xml_document by read_html()
url <- "https://www.ptt.cc/bbs/HatePolitics/index.html"
doc <- GET(url, config = set_cookies("over18" = "1")) %>%
    content("text") %>%
    read_html()



# write_html() again to final checking
doc %>% write_html("test_with_cookie.html")


```


# Parse html

```{r}
# GET() all nodes
nodes <- html_nodes(doc, ".r-ent")
length(nodes)

# For all nodes, retrieve number of recommendation to var nrec
nrec <- html_node(nodes, ".nrec span") %>% html_text() %>% as.numeric()
nrec
# For all nodes, retrieve title to variable title
title <- html_node(nodes, ".title a") %>% html_text()
title

# For all nodes, retrieve link to variable link
# Remember to paste the prefix link to the link
# Try to browse it for verification
pre <- "https://www.ptt.cc"
link <- html_node(nodes, ".title a") %>% html_attr("href") %>%
    str_c(pre, .)
link[1] %>% browseURL()

# For all nodes, retrieve author to variable author
author <- html_node(nodes, ".meta .author") %>% html_text()
author


# Combine all variable as data.frame
page.df <- data_frame(nrec, title, link, author)


```



# Formatting the url
```{r}
url <- "https://www.ptt.cc/bbs/HatePolitics/search?page=1&q=林昶佐"
# the query -> query
query <- "林昶佐"

# the prefixed url -> pre
pre <- "https://www.ptt.cc"

# the url by pasting the url, page number and the query
url <- str_c("https://www.ptt.cc/bbs/HatePolitics/search?page=", 
             1,
             "&q=",
             query)
# preview the url
url
browseURL(url)
```


# Using for-loop to get back all pages
```{r}
# the query
query = "林昶佐"
pre <- "https://www.ptt.cc"

# Creating an empty data frame by data_frame()
post.df <- data_frame()

# for-loop
for(page in 1:8){
    url <- str_c("https://www.ptt.cc/bbs/HatePolitics/search?page=", 
             page,
             "&q=",
             query)
    print(url)
    doc <- GET(url, config = set_cookies("over18" = "1")) %>%
        content("text") %>%
        read_html()
    nodes <- html_nodes(doc, ".r-ent")
    nrec <- html_node(nodes, ".nrec span") %>% html_text() %>% as.numeric()
    title <- html_node(nodes, ".title a") %>% html_text()
    link <- html_node(nodes, ".title a") %>% html_attr("href") %>%
        str_c(pre, .)
    author <- html_node(nodes, ".meta .author") %>% html_text()
    page.df <- data_frame(nrec, title, link, author)
    post.df <- bind_rows(post.df, page.df)
    message(nrow(post.df))
}
post.df %>% write_rds("post_HatePolitics_lin.rds")











```


# NOTES and FURTHER
Now we detect the last page number manually. You can try to write a function to crawl back all data given a board name and a query. One more thing you need to think by yourself is that you need to detect the last page number automatically. Try to do it!


